{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abdi Seq2Seq (Input -> Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq_Abdi\n",
    " \n",
    "- Dataset: Movies Lines\n",
    "- Lines: 25,141\n",
    "- Samples Size: 20,000 \n",
    "- Vocab Size: 9,000 \n",
    "- Epochs: 30\n",
    "- Acc: 99%\n",
    "- Loss: 0.0049\n",
    "    \n",
    "by Abdi (Sep, 2019)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_r70epHozOt"
   },
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK2TWV1nm48Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print( tf.VERSION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables Initialization\n",
    "VOCAB_SIZE = 9000\n",
    "MAX_LEN = 20\n",
    "SAMPLES_SIZE = 20000\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 256\n",
    "EMB_DIM = 256\n",
    "\n",
    "class color:\n",
    "        PURPLE = '\\033[95m'\n",
    "        CYAN = '\\033[96m'\n",
    "        DARKCYAN = '\\033[36m'\n",
    "        BLUE = '\\033[94m'\n",
    "        GREEN = '\\033[92m'\n",
    "        YELLOW = '\\033[93m'\n",
    "        RED = '\\033[91m'\n",
    "        BOLD = '\\033[1m'\n",
    "        UNDERLINE = '\\033[4m'\n",
    "        END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (25141, 4)\n",
      "Load completed!\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load the dataset\n",
    "def load_dataset():\n",
    "    \n",
    "    import glob\n",
    "\n",
    "    path = r'Dataset'\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    li = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    movies = pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"Data shape:\", movies.shape)\n",
    "    print(\"Load completed!\")\n",
    "    print(\"---------\\n\")\n",
    "    \n",
    "    return movies\n",
    "\n",
    "movies = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean text function\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>actors</th>\n",
       "      <th>to_who</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>please luke please please please</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>LUKE</td>\n",
       "      <td>gilmore girls,  season : 01, episode : 01</td>\n",
       "      <td>please luke please please please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>how many cups have you had this morning</td>\n",
       "      <td>LUKE</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>gilmore girls,  season : 01, episode : 01</td>\n",
       "      <td>how many cups have you had this morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>gilmore girls,  season : 01, episode : 01</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>plus</td>\n",
       "      <td>LUKE</td>\n",
       "      <td>LUKE</td>\n",
       "      <td>gilmore girls,  season : 01, episode : 01</td>\n",
       "      <td>plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>five but yours is better</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>LORELAI</td>\n",
       "      <td>gilmore girls,  season : 01, episode : 01</td>\n",
       "      <td>five but yours is better</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     input   actors   to_who  \\\n",
       "0        please luke please please please   LORELAI     LUKE   \n",
       "1  how many cups have you had this morning     LUKE  LORELAI   \n",
       "2                                     none  LORELAI  LORELAI   \n",
       "3                                     plus     LUKE     LUKE   \n",
       "4                five but yours is better   LORELAI  LORELAI   \n",
       "\n",
       "                                  movie_name  \\\n",
       "0  gilmore girls,  season : 01, episode : 01   \n",
       "1  gilmore girls,  season : 01, episode : 01   \n",
       "2  gilmore girls,  season : 01, episode : 01   \n",
       "3  gilmore girls,  season : 01, episode : 01   \n",
       "4  gilmore girls,  season : 01, episode : 01   \n",
       "\n",
       "                                    target  \n",
       "0        please luke please please please   \n",
       "1  how many cups have you had this morning  \n",
       "2                                     none  \n",
       "3                                     plus  \n",
       "4                five but yours is better   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pre-processing\n",
    "target = movies['sentences']\n",
    "target = target.reset_index(drop=True)\n",
    "movies['target'] = target\n",
    "movies.rename(columns={'sentences':'input'}, inplace=True)\n",
    "movies.dropna(axis=0, how='any', inplace=True)\n",
    "movies = movies.iloc[ : SAMPLES_SIZE] \n",
    "print(movies.shape)\n",
    "\n",
    "movies['input'] = movies['input'].apply(clean_text)\n",
    "movies['target'] = movies['target'].apply(clean_text)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2_ux1rZnDyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input max length is: 280\n",
      "Encoder input data shape: (20000, 20)\n",
      "Number of Input tokens: 9000\n"
     ]
    }
   ],
   "source": [
    "### INPUTS\n",
    "input_lines = list()\n",
    "for line in movies.input:\n",
    "    input_lines.append(line) \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words = VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(input_lines ) \n",
    "tokenized_input_lines = tokenizer.texts_to_sequences(input_lines) \n",
    "\n",
    "### Arrumar aqui\n",
    "length_list = list()\n",
    "for token_seq in tokenized_input_lines:\n",
    "    length_list.append(len(token_seq))\n",
    "    max_input_length = max(length_list) # Gets the higher value in the list   \n",
    "print('Input max length is:', max_input_length)\n",
    "\n",
    "padded_input_lines = preprocessing.sequence.pad_sequences(tokenized_input_lines , maxlen= MAX_LEN , padding='post')\n",
    "encoder_input_data = np.array(padded_input_lines)\n",
    "print('Encoder input data shape:', encoder_input_data.shape)\n",
    "\n",
    "input_word_dict = tokenizer.word_index\n",
    "input_word_dict = dict(itertools.islice(input_word_dict.items(), VOCAB_SIZE-1))\n",
    "num_input_tokens = len(input_word_dict)+1\n",
    "print('Number of Input tokens:', num_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deB0oX_0pj8R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target max length is: 282\n",
      "Decoder input data shape: (20000, 20)\n",
      "Number of Target tokens: 9000\n"
     ]
    }
   ],
   "source": [
    "### TARGETS\n",
    "target_lines = list()\n",
    "for line in movies.target:\n",
    "    target_lines.append('<BOS> ' + line + ' <EOS>')  \n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words = VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(target_lines) \n",
    "tokenized_target_lines = tokenizer.texts_to_sequences(target_lines) \n",
    "\n",
    "length_list = list()\n",
    "for token_seq in tokenized_target_lines:\n",
    "    length_list.append(len(token_seq))\n",
    "    max_target_length = max(length_list) # Gets the higher value in the list   \n",
    "print('Target max length is:', max_target_length)\n",
    "\n",
    "padded_target_lines = preprocessing.sequence.pad_sequences(tokenized_target_lines , maxlen = MAX_LEN, padding='post' )\n",
    "decoder_input_data = np.array(padded_target_lines)\n",
    "print('Decoder input data shape:', decoder_input_data.shape)\n",
    "\n",
    "\n",
    "target_word_dict = tokenizer.word_index\n",
    "target_word_dict = dict(itertools.islice(target_word_dict.items(), VOCAB_SIZE-1))\n",
    "num_target_tokens = len(target_word_dict)+1\n",
    "print('Number of Target tokens:', num_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPCTmeL7qj3T",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder target data shape: (20000, 20, 9000)\n"
     ]
    }
   ],
   "source": [
    "### Prepare the decoder target data\n",
    "decoder_target_data = list()\n",
    "\n",
    "for token_seq in tokenized_target_lines:\n",
    "    decoder_target_data.append(token_seq[1:]) \n",
    "    \n",
    "padded_target_lines = preprocessing.sequence.pad_sequences(decoder_target_data , maxlen = MAX_LEN, padding='post')\n",
    "decoder_target_data = utils.to_categorical(padded_target_lines, VOCAB_SIZE)\n",
    "\n",
    "\n",
    "print('Decoder target data shape:', decoder_target_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the embedding matrix\n",
    "def embedding_matrix_creator(EMB_DIM, embeddings_index):\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, EMB_DIM))\n",
    "    for word, i in target_word_dict.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:      \n",
    "            embedding_matrix[i] = embedding_vector # words not found in embedding index will be all-zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "### Load GloVe\n",
    "def embedding_master():\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    path = r'C:\\Users\\abdi\\Desktop\\2Meu\\WordEmbeddings'\n",
    "    #with open(path + 'glove.6B.50d.txt', encoding='utf-8') as f: # 50 dimension, 400,000 words\n",
    "    with open(path + '\\glove.42B.300d.txt', encoding='utf-8') as f:# 300 dimension 1,900,000 words\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "    print(\"Glove Loaded!\") \n",
    "    \n",
    "    from keras.layers import Embedding\n",
    "    \n",
    "    ### Define the embedding layer\n",
    "    embedding_matrix = embedding_matrix_creator(EMB_DIM, embeddings_index) # change embedding dimensions\n",
    "    embed_layer = tf.keras.layers.Embedding(input_dim = VOCAB_SIZE, \n",
    "                        output_dim = EMB_DIM, \n",
    "                        input_length = MAX_LEN, \n",
    "                        weights = [embedding_matrix], trainable =  True, mask_zero = True) # True for dynamic | False for static \n",
    "    \n",
    "    print(\"Embedded layer completed!\")\n",
    "    print(\"---------\\n\")\n",
    "    \n",
    "    return embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KS5gWlcpFT1"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "Hqb4Bps1s_Lr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    2304000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2304000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9000)   2313000     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,971,624\n",
      "Trainable params: 7,971,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_DIM, mask_zero=True )(encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(HIDDEN_DIM, return_state = True , recurrent_dropout = 0.1 , dropout = 0.1 )(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_DIM, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(HIDDEN_DIM, return_state = True , return_sequences=True , recurrent_dropout = 0.1 , dropout = 0.1)\n",
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense (decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "fnd2H27qt4Hy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, \n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    epochs = EPOCHS,\n",
    "                    validation_split = 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eeqv_vH5pMpb"
   },
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNhVkiZLvdTq"
   },
   "outputs": [],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(HIDDEN_DIM,))    \n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(HIDDEN_DIM,))\n",
    "    \n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "    \n",
    "enc_model, dec_model = encoder_model, decoder_model\n",
    "\n",
    "#enc_model.save( 'enc_model_B.h5' ) \n",
    "#dec_model.save( 'dec_model_B.h5' ) \n",
    "#model.save( 'model_B.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_hrJcNP-mXb"
   },
   "outputs": [],
   "source": [
    "def encoding_input( sentence : str ):\n",
    "    sentence = clean_text(sentence)\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append(input_word_dict[ word ]) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen = MAX_LEN , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "2Mfco9WKukhS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: you are very awesome\n",
      "\u001b[92mInput:\n",
      "\u001b[0m you are very awesome\n",
      "\u001b[94mPrediction:\n",
      "\u001b[0m you are very rude \n",
      "\u001b[94mVector:\n",
      "\u001b[0m [4, 13, 94, 1370]\n",
      "\n",
      "\n",
      "Enter a sentence: oh you know what actually i am meeting someone\n",
      "\u001b[92mInput:\n",
      "\u001b[0m oh you know what actually i am meeting someone\n",
      "\u001b[94mPrediction:\n",
      "\u001b[0m oh you know what actually i am meeting someone \n",
      "\u001b[94mVector:\n",
      "\u001b[0m [26, 4, 23, 18, 187, 3, 15, 564, 348]\n",
      "\n",
      "\n",
      "Enter a sentence: oh you know what actually i am going home today\n",
      "\u001b[92mInput:\n",
      "\u001b[0m oh you know what actually i am going home today\n",
      "\u001b[94mPrediction:\n",
      "\u001b[0m oh you know what actually i am going to go \n",
      "\u001b[94mVector:\n",
      "\u001b[0m [26, 4, 23, 18, 187, 3, 15, 69, 7, 44]\n",
      "\n",
      "\n",
      "Enter a sentence: I am thinking about a new world\n",
      "\u001b[92mInput:\n",
      "\u001b[0m I am thinking about a new world\n",
      "\u001b[94mPrediction:\n",
      "\u001b[0m i am thinking about a new world \n",
      "\u001b[94mVector:\n",
      "\u001b[0m [3, 15, 275, 45, 8, 182, 344]\n",
      "\n",
      "\n",
      "Enter a sentence: I am very curious about the presents on this christmas\n",
      "\u001b[92mInput:\n",
      "\u001b[0m I am very curious about the presents on this christmas\n",
      "\u001b[94mPrediction:\n",
      "\u001b[0m i am very pleased about this whole humidity thing it \n",
      "\u001b[94mVector:\n",
      "\u001b[0m [3, 15, 94, 1746, 45, 22, 220, 5830, 103, 9]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Inference\n",
    "for epoch in range(5):\n",
    "    input_seq = input('Enter a sentence: ')\n",
    "    states_values = enc_model.predict(encoding_input(input_seq))\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = target_word_dict['bos']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    decoded_vector = []\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :] ) ### Prediction Vector\n",
    "        sampled_word = None\n",
    "        \n",
    "        for word , index in target_word_dict.items() :\n",
    "            if sampled_word_index == index :                \n",
    "                sampled_word = word \n",
    "                #print(sampled_word)\n",
    "                \n",
    "                if sampled_word == 'eos' or len(decoded_translation.split()) > MAX_LEN:\n",
    "                    stop_condition = True\n",
    "                    \n",
    "                else: \n",
    "                    decoded_translation += word + ' '\n",
    "                    decoded_vector.append(sampled_word_index)\n",
    "                    \n",
    "                              \n",
    "        empty_target_seq = np.zeros(( 1 , 1 ))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    \n",
    "    \n",
    "    print(color.GREEN + \"Input:\\n\" + color.END, input_seq)\n",
    "    print(color.BLUE + \"Prediction:\\n\" + color.END, decoded_translation)\n",
    "    print(color.BLUE + \"Vector:\\n\" + color.END, decoded_vector)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_plot(history):\n",
    "    #plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    #plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "#performance_plot(history)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\abdi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    model = tf.keras.models.load_model('model_B.h5')\n",
    "    enc_model = tf.keras.models.load_model('enc_model_B.h5') \n",
    "    dec_model = tf.keras.models.load_model('dec_model_B.h5')   \n",
    "    \n",
    "    return model, enc_model, dec_model\n",
    "\n",
    "model, enc_model, dec_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural_Machine_Translation_( Eng-French ).ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "11os3isH4I4X76dwOAQJ5cSRnfhmUziHm",
     "timestamp": 1552703269935
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
